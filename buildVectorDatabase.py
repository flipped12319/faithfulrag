import pickle

import faiss
from datasets import load_dataset, load_from_disk
from sentence_transformers import SentenceTransformer
import numpy as np
dataset = load_from_disk("chunk_dataset")
model = SentenceTransformer("Qwen/Qwen3-Embedding-0.6B")
print(dataset)
texts, ids = [], []
for item in dataset:
    for chunk in item["chunks"]:
        texts.append(chunk)
        ids.append(item["id"])   # âœ… ä¿ç•™æ¥æº id

print(f"ä¸€å…± {len(texts)} ä¸ªæ–‡æœ¬å—")
print(ids)
embeddings = model.encode(
    texts,
    batch_size=32,             # âœ… é˜²æ­¢æ˜¾å­˜æº¢å‡º
    show_progress_bar=True,    # âœ… æ˜¾ç¤ºè¿›åº¦
    convert_to_numpy=True,     # âœ… è½¬æˆ numpyï¼Œæ–¹ä¾¿å­˜ FAISS
    normalize_embeddings=True  # âœ… è‹¥åç»­ç”¨å†…ç§¯æœç´¢ï¼Œç›¸å½“äºä½™å¼¦ç›¸ä¼¼åº¦
)
print(f"å‘é‡ shape: {embeddings.shape}")

# index = faiss.IndexFlatL2(dimension)
embeddings = np.array(embeddings).astype("float32")
dimension = embeddings.shape[1]
index = faiss.IndexFlatIP(dimension)
index.add(embeddings)

print(f"âœ… å‘é‡åº“ä¸­å…±æœ‰ {index.ntotal} æ¡å‘é‡")

# 6ï¸âƒ£ ä¿å­˜ç´¢å¼•ä¸å…ƒæ•°æ®
faiss.write_index(index, "chunk_index.faiss")
with open("chunk_metadata.pkl", "wb") as f:
    pickle.dump({"texts": texts, "ids": ids}, f)

print("ğŸ’¾ å·²ä¿å­˜ FAISS ç´¢å¼•ä¸å…ƒæ•°æ®ï¼")